# Gradient Descent Optimizers Demo in TensorFlow

This project compares different gradient descent optimizers in TensorFlow on a simple insurance dataset.

## Optimizers Tested
- SGD
- SGD + Momentum
- SGD + Nesterov
- Adam
- Adamax
- Nadam
- RMSprop
- Adagrad
- Adadelta
- Ftrl

## How to Run
```bash
pip install -r requirements.txt
python optimizers_demo.py
